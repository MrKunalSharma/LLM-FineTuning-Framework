# Model Configuration
model:
  name: "microsoft/phi-2"  # Using Phi-2 for efficient training
  max_length: 512
  temperature: 0.7
  top_p: 0.9
  device: "cuda" # Will auto-fallback to CPU
  
# Training Configuration  
training:
  output_dir: "./models/finetuned"
  num_epochs: 3
  learning_rate: 2e-5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  warmup_steps: 100
  weight_decay: 0.01
  logging_dir: "./logs"
  save_steps: 500
  eval_steps: 100
  
# LoRA Configuration
lora:
  r: 16
  lora_alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "dense"]
  lora_dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"
  
# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: true

# Wandb Configuration
wandb:
  project: "llm-finetuning"
  entity: null  # Add your wandb username
  tags: ["phi-2", "lora", "finetuning"]
